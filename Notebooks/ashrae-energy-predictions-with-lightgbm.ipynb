{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.027039,
     "end_time": "2021-01-03T05:10:18.440603",
     "exception": false,
     "start_time": "2021-01-03T05:10:18.413564",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook a model is generated for predicting building energy peformance as set out in the ASHRAE - Great Energy Predictor III competition. The analysis proceeds as follows:\n",
    "\n",
    "<a href='#1'>1. Loading Data</a>\n",
    "\n",
    "<a href='#2'>2. Missing Values</a>\n",
    "\n",
    "<a href='#3'>3. Combining Datasets</a>\n",
    "\n",
    "   <a href='#3.1'>3.1. Additional Missing Data</a>\n",
    "    \n",
    "<a href='#4'>4. Reducing Memory Requirements</a>\n",
    "\n",
    "<a href='#5'>5. Adding Features</a>\n",
    "\n",
    "<a href='#6'>6. Outlier Treatment</a>\n",
    "\n",
    "<a href='#7'>7. Model Training</a>\n",
    "\n",
    "<a href='#8'>8. Model Predictions</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.024962,
     "end_time": "2021-01-03T05:10:18.490718",
     "exception": false,
     "start_time": "2021-01-03T05:10:18.465756",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <a id='1'>1. Loading Data</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-03T05:10:18.549505Z",
     "iopub.status.busy": "2021-01-03T05:10:18.548635Z",
     "iopub.status.idle": "2021-01-03T05:10:19.507518Z",
     "shell.execute_reply": "2021-01-03T05:10:19.506920Z"
    },
    "papermill": {
     "duration": 0.991545,
     "end_time": "2021-01-03T05:10:19.507633",
     "exception": false,
     "start_time": "2021-01-03T05:10:18.516088",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import missingno as msno \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-03T05:10:19.570340Z",
     "iopub.status.busy": "2021-01-03T05:10:19.569667Z",
     "iopub.status.idle": "2021-01-03T05:11:20.056597Z",
     "shell.execute_reply": "2021-01-03T05:11:20.055890Z"
    },
    "papermill": {
     "duration": 60.522832,
     "end_time": "2021-01-03T05:11:20.056708",
     "exception": false,
     "start_time": "2021-01-03T05:10:19.533876",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File /kaggle/input/ashrae-energy-prediction/train.csv does not exist: '/kaggle/input/ashrae-energy-prediction/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-09a69386b64a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#load data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/kaggle/input/ashrae-energy-prediction/train.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'timestamp'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'timestamp'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#the train dataset contains a 'timestamp' column we convert to a datetime object for ease of use\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/kaggle/input/ashrae-energy-prediction/test.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'timestamp'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'timestamp'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dhdsblend\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dhdsblend\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dhdsblend\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dhdsblend\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dhdsblend\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File /kaggle/input/ashrae-energy-prediction/train.csv does not exist: '/kaggle/input/ashrae-energy-prediction/train.csv'"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "train = pd.read_csv('/kaggle/input/ashrae-energy-prediction/train.csv')\n",
    "train['timestamp'] = pd.to_datetime(train['timestamp']) #the train dataset contains a 'timestamp' column we convert to a datetime object for ease of use\n",
    "test = pd.read_csv('/kaggle/input/ashrae-energy-prediction/test.csv')\n",
    "test['timestamp'] = pd.to_datetime(test['timestamp'])\n",
    "weather_train = pd.read_csv('/kaggle/input/ashrae-energy-prediction/weather_train.csv')\n",
    "weather_test = pd.read_csv('/kaggle/input/ashrae-energy-prediction/weather_test.csv')\n",
    "build_meta = pd.read_csv('/kaggle/input/ashrae-energy-prediction/building_metadata.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.02567,
     "end_time": "2021-01-03T05:11:20.108868",
     "exception": false,
     "start_time": "2021-01-03T05:11:20.083198",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <a id='2'>2. Missing Values</a>\n",
    "\n",
    "The train and test data contains missing values and is spread out over several dataframes (meter readings, building meta-data, and weather data). Here we fill the missing values and combine the datsets into overall train/test sets for model training and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-03T05:11:20.171740Z",
     "iopub.status.busy": "2021-01-03T05:11:20.171136Z",
     "iopub.status.idle": "2021-01-03T05:11:20.174573Z",
     "shell.execute_reply": "2021-01-03T05:11:20.174042Z"
    },
    "papermill": {
     "duration": 0.039382,
     "end_time": "2021-01-03T05:11:20.174685",
     "exception": false,
     "start_time": "2021-01-03T05:11:20.135303",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Define a function to check if there are is any missing information in the datasets\n",
    "def get_missing_info(df):\n",
    "    num_entries = df.shape[0]*df.shape[1]\n",
    "    null_entries = df.isnull().sum().sum()\n",
    "    percent_empty = null_entries/num_entries*100\n",
    "    num_missing = df.isna().sum()\n",
    "    percent_missing = num_missing/len(df)*100\n",
    "    col_modes = df.mode().loc[0]\n",
    "    percent_mode = [df[x].isin([df[x].mode()[0]]).sum()/len(df)*100 for x in df]\n",
    "    missing_value_df = pd.DataFrame({'num_missing': num_missing,\n",
    "                                     'percent_missing': percent_missing, \n",
    "                                     'mode': col_modes,\n",
    "                                     'percent_mode':percent_mode})\n",
    "    print('total empty percent:', percent_empty, '%')\n",
    "    print('columns that are more than 97% mode:', missing_value_df.loc[missing_value_df['percent_mode']>97].index.values)\n",
    "    return(missing_value_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-03T05:11:20.233410Z",
     "iopub.status.busy": "2021-01-03T05:11:20.232732Z",
     "iopub.status.idle": "2021-01-03T05:11:25.174623Z",
     "shell.execute_reply": "2021-01-03T05:11:25.174095Z"
    },
    "papermill": {
     "duration": 4.973423,
     "end_time": "2021-01-03T05:11:25.174731",
     "exception": false,
     "start_time": "2021-01-03T05:11:20.201308",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_missing_info(train)\n",
    "# get_missing_info(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.027575,
     "end_time": "2021-01-03T05:11:25.229828",
     "exception": false,
     "start_time": "2021-01-03T05:11:25.202253",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "There are no missing entries in the train or test dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-03T05:11:25.294123Z",
     "iopub.status.busy": "2021-01-03T05:11:25.293444Z",
     "iopub.status.idle": "2021-01-03T05:11:25.323611Z",
     "shell.execute_reply": "2021-01-03T05:11:25.323065Z"
    },
    "papermill": {
     "duration": 0.065359,
     "end_time": "2021-01-03T05:11:25.323742",
     "exception": false,
     "start_time": "2021-01-03T05:11:25.258383",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_missing_info(build_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-03T05:11:25.388463Z",
     "iopub.status.busy": "2021-01-03T05:11:25.385240Z",
     "iopub.status.idle": "2021-01-03T05:11:25.391849Z",
     "shell.execute_reply": "2021-01-03T05:11:25.391254Z"
    },
    "papermill": {
     "duration": 0.039701,
     "end_time": "2021-01-03T05:11:25.391949",
     "exception": false,
     "start_time": "2021-01-03T05:11:25.352248",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Filling in missing values in the building meta-data column\n",
    "#Make a copy so we are not changing the initial data\n",
    "build_meta_f = build_meta.copy()\n",
    "#fill all the missing floor counts by the mode (1) and the missing year built by the mean. Nothing else is missing\n",
    "build_meta_f.fillna({'floor_count':1, 'year_built':int(build_meta['year_built'].mean())}, inplace=True) \n",
    "#this is the only categorical column. Convert so it can be handled later by lgbm during fitting\n",
    "build_meta_f['primary_use'] = build_meta_f['primary_use'].astype('category') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-03T05:11:25.452639Z",
     "iopub.status.busy": "2021-01-03T05:11:25.452024Z",
     "iopub.status.idle": "2021-01-03T05:11:25.586755Z",
     "shell.execute_reply": "2021-01-03T05:11:25.586216Z"
    },
    "papermill": {
     "duration": 0.166944,
     "end_time": "2021-01-03T05:11:25.586863",
     "exception": false,
     "start_time": "2021-01-03T05:11:25.419919",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_missing_info(weather_train)\n",
    "# get_missing_info(weather_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.031005,
     "end_time": "2021-01-03T05:11:25.647358",
     "exception": false,
     "start_time": "2021-01-03T05:11:25.616353",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "When filling the missing weather data, using previous or future measurements can be good provided they are not too far in the future or past. We will start with forward/back filling weather data up to 24 hours for each site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-03T05:11:25.718410Z",
     "iopub.status.busy": "2021-01-03T05:11:25.717510Z",
     "iopub.status.idle": "2021-01-03T05:11:25.906935Z",
     "shell.execute_reply": "2021-01-03T05:11:25.906170Z"
    },
    "papermill": {
     "duration": 0.228177,
     "end_time": "2021-01-03T05:11:25.907090",
     "exception": false,
     "start_time": "2021-01-03T05:11:25.678913",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Forward filling missing data in the weather dataset +-24 hours\n",
    "#Train weather\n",
    "weather_train_f = weather_train.copy() #make a copy so we aren't changing our oridinal data\n",
    "weather_train_f['timestamp'] = pd.to_datetime(weather_train_f['timestamp']) #turn the timestamp column into a datetime object\n",
    "weather_train_f = weather_train_f.sort_values(by=['site_id', 'timestamp']) #short values by site id then timestamp\n",
    "\n",
    "#test weather\n",
    "weather_test_f = weather_test.copy() #make a copy so we aren't changing our oridinal data\n",
    "weather_test_f['timestamp'] = pd.to_datetime(weather_test_f['timestamp']) #turn the timestamp column into a datetime object\n",
    "weather_test_f = weather_test_f.sort_values(by=['site_id', 'timestamp']) #short values by site id then timestamp\n",
    "\n",
    "weather_train_f.fillna(method = 'ffill', inplace=True, limit = 24)#forward fill the missing data up to 12 hours\n",
    "weather_train_f.fillna(method = 'bfill', inplace=True, limit = 24)#backfill up to 12 hours\n",
    "\n",
    "weather_test_f.fillna(method = 'ffill', inplace=True, limit = 24)#forward fill the missing data up to 12 hours\n",
    "weather_test_f.fillna(method = 'bfill', inplace=True, limit = 24)#forward fill the missing data up to 12 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-03T05:11:26.000868Z",
     "iopub.status.busy": "2021-01-03T05:11:25.999289Z",
     "iopub.status.idle": "2021-01-03T05:11:26.677753Z",
     "shell.execute_reply": "2021-01-03T05:11:26.678284Z"
    },
    "papermill": {
     "duration": 0.727556,
     "end_time": "2021-01-03T05:11:26.678421",
     "exception": false,
     "start_time": "2021-01-03T05:11:25.950865",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "msno.matrix(weather_train_f)\n",
    "# msno.matrix(weather_test_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.031177,
     "end_time": "2021-01-03T05:11:26.741513",
     "exception": false,
     "start_time": "2021-01-03T05:11:26.710336",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As seen above, there are still blocks of data missing over long periods of time. For these we will fill with the mean values for the given site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-03T05:11:26.824446Z",
     "iopub.status.busy": "2021-01-03T05:11:26.823491Z",
     "iopub.status.idle": "2021-01-03T05:11:26.900049Z",
     "shell.execute_reply": "2021-01-03T05:11:26.899467Z"
    },
    "papermill": {
     "duration": 0.127253,
     "end_time": "2021-01-03T05:11:26.900160",
     "exception": false,
     "start_time": "2021-01-03T05:11:26.772907",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Train data\n",
    "missing_cols = [col for col in weather_train_f.columns if weather_train_f[col].isna().any()] \n",
    "fill_lib = weather_train_f.groupby('site_id')[missing_cols].transform('mean')#stores the mean of each feature for each site id\n",
    "weather_train_f.fillna(fill_lib, inplace=True) #for each feature with missing values, fill the missing entry with the mean for that site\n",
    "\n",
    "#Test data\n",
    "missing_cols = [col for col in weather_test_f.columns if weather_test_f[col].isna().any()]\n",
    "fill_lib = weather_test_f.groupby('site_id')[missing_cols].transform('mean')\n",
    "weather_test_f.fillna(fill_lib, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.03112,
     "end_time": "2021-01-03T05:11:26.963796",
     "exception": false,
     "start_time": "2021-01-03T05:11:26.932676",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <a id='3'>3. Combining Datasets</a>\n",
    "Now that we have filled in all the missing data, we will merge everything into train and test dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-03T05:11:27.060382Z",
     "iopub.status.busy": "2021-01-03T05:11:27.059248Z",
     "iopub.status.idle": "2021-01-03T05:11:50.457295Z",
     "shell.execute_reply": "2021-01-03T05:11:50.456609Z"
    },
    "papermill": {
     "duration": 23.457306,
     "end_time": "2021-01-03T05:11:50.457416",
     "exception": false,
     "start_time": "2021-01-03T05:11:27.000110",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#merge the building meta data and weather data into the train data\n",
    "train_m = train.merge(build_meta_f, how='left', on = ['building_id'], validate='many_to_one') #merge the building meta data into the train data\n",
    "train_m = train_m.merge(weather_train_f, how='left', on = ['site_id', 'timestamp'], validate='many_to_one')#add weather data to each time entry for each site ID\n",
    "\n",
    "#merge the building meta data and weather data into the test data\n",
    "test_m = test.merge(build_meta_f, how='left', on = ['building_id'], validate='many_to_one') #merge the building meta data into the train data\n",
    "test_m = test_m.merge(weather_test_f, how='left', on = ['site_id', 'timestamp'], validate='many_to_one')#add weather data to each time entry for each site ID\n",
    "\n",
    "#we now delete some of the dataframes we no longer need to free up memory\n",
    "del train, test, weather_train, weather_test, weather_train_f, weather_test_f, build_meta\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "train_m.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.032284,
     "end_time": "2021-01-03T05:11:50.522776",
     "exception": false,
     "start_time": "2021-01-03T05:11:50.490492",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='3.1'>3.1. Additional Missing Data</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-03T05:11:50.592613Z",
     "iopub.status.busy": "2021-01-03T05:11:50.591942Z",
     "iopub.status.idle": "2021-01-03T05:12:02.427549Z",
     "shell.execute_reply": "2021-01-03T05:12:02.428216Z"
    },
    "papermill": {
     "duration": 11.873173,
     "end_time": "2021-01-03T05:12:02.428390",
     "exception": false,
     "start_time": "2021-01-03T05:11:50.555217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_missing_info(train_m)\n",
    "#get_missing_info(test_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.032745,
     "end_time": "2021-01-03T05:12:02.495280",
     "exception": false,
     "start_time": "2021-01-03T05:12:02.462535",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We can now see some additional missing data in the merged datasets from timestamps present in the train/test sets that\n",
    "were not in the weather data. We will forward fill this data for each building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-03T05:12:02.568308Z",
     "iopub.status.busy": "2021-01-03T05:12:02.567474Z",
     "iopub.status.idle": "2021-01-03T05:12:53.040295Z",
     "shell.execute_reply": "2021-01-03T05:12:53.039394Z"
    },
    "papermill": {
     "duration": 50.512742,
     "end_time": "2021-01-03T05:12:53.040432",
     "exception": false,
     "start_time": "2021-01-03T05:12:02.527690",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#train_data\n",
    "train_m = train_m.sort_values(by=['building_id', 'timestamp'])\n",
    "train_m.fillna(method = 'ffill', inplace=True)\n",
    "\n",
    "#we need to temporarily remove the train_m datset from memory here so we do not run out of memory\n",
    "pickle.dump( train_m, open( \"train_m.p\", \"wb\" ) )\n",
    "del train_m\n",
    "\n",
    "#test data\n",
    "test_m = test_m.sort_values(by=['building_id', 'timestamp'])\n",
    "test_m.fillna(method = 'ffill', inplace=True)\n",
    "\n",
    "train_m = pickle.load( open('train_m.p', 'rb')) #load back in the train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.04973,
     "end_time": "2021-01-03T05:12:53.142432",
     "exception": false,
     "start_time": "2021-01-03T05:12:53.092702",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <a id='4'>4. Reducing Memory Requirements</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.049959,
     "end_time": "2021-01-03T05:12:53.242831",
     "exception": false,
     "start_time": "2021-01-03T05:12:53.192872",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We can reduce the size of our data by changing the datatype based on the precision required by the numbers. Below we use a custom function to achieve this. Source: https://www.kaggle.com/caesarlupum/ashrae-start-here-a-gentle-introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2021-01-03T05:12:53.358352Z",
     "iopub.status.busy": "2021-01-03T05:12:53.353162Z",
     "iopub.status.idle": "2021-01-03T05:13:04.949662Z",
     "shell.execute_reply": "2021-01-03T05:13:04.949047Z"
    },
    "papermill": {
     "duration": 11.655762,
     "end_time": "2021-01-03T05:13:04.949779",
     "exception": false,
     "start_time": "2021-01-03T05:12:53.294017",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "train_m = reduce_mem_usage(train_m)\n",
    "test_m = reduce_mem_usage(test_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.034027,
     "end_time": "2021-01-03T05:13:05.017749",
     "exception": false,
     "start_time": "2021-01-03T05:13:04.983722",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <a id='5'>5. Adding Features</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-03T05:13:05.094610Z",
     "iopub.status.busy": "2021-01-03T05:13:05.093772Z",
     "iopub.status.idle": "2021-01-03T05:13:15.248039Z",
     "shell.execute_reply": "2021-01-03T05:13:15.247154Z"
    },
    "papermill": {
     "duration": 10.196322,
     "end_time": "2021-01-03T05:13:15.248178",
     "exception": false,
     "start_time": "2021-01-03T05:13:05.051856",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Add hour, time of year, and weekend columns\n",
    "#Train\n",
    "train_m['hour'] = train_m['timestamp'].dt.hour\n",
    "train_m['day_of_year'] = (train_m['timestamp'] - pd.Timestamp('2016-01-01')).dt.days%365\n",
    "train_m['is_weekend'] = train_m['timestamp'].dt.weekday.isin([5,6]).astype(int)\n",
    "\n",
    "#Test\n",
    "test_m['hour'] = test_m['timestamp'].dt.hour\n",
    "test_m['day_of_year'] = (test_m['timestamp'] - pd.Timestamp('2016-01-01')).dt.days%365\n",
    "test_m['is_weekend'] = test_m['timestamp'].dt.weekday.isin([5,6]).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.051157,
     "end_time": "2021-01-03T05:13:15.351349",
     "exception": false,
     "start_time": "2021-01-03T05:13:15.300192",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <a id='6'>6. Outlier Treatment</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.050321,
     "end_time": "2021-01-03T05:13:15.452668",
     "exception": false,
     "start_time": "2021-01-03T05:13:15.402347",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Below we look at the buildings which tend to have the highest meter readings for each meter type. As we can see, relatively few buildings tend to dominate in terms of meter reading. As an example, building 803 has an average meter reading 0 roughly 20x the average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-03T05:13:15.561511Z",
     "iopub.status.busy": "2021-01-03T05:13:15.560843Z",
     "iopub.status.idle": "2021-01-03T05:13:25.512537Z",
     "shell.execute_reply": "2021-01-03T05:13:25.513046Z"
    },
    "papermill": {
     "duration": 10.009653,
     "end_time": "2021-01-03T05:13:25.513194",
     "exception": false,
     "start_time": "2021-01-03T05:13:15.503541",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for m in range(4):\n",
    "    idxm = train_m[train_m['meter']==m].groupby('timestamp')['meter_reading'].idxmax()#index of max meter reading for the timestamp\n",
    "    print('meter {}'.format(m))\n",
    "    #print the number of hours the building was the highest consumer for the top 5 buildings\n",
    "    print(train_m.loc[idxm, 'building_id'].value_counts().iloc[:5]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-03T05:13:25.593141Z",
     "iopub.status.busy": "2021-01-03T05:13:25.592507Z",
     "iopub.status.idle": "2021-01-03T05:13:26.384286Z",
     "shell.execute_reply": "2021-01-03T05:13:26.383431Z"
    },
    "papermill": {
     "duration": 0.83476,
     "end_time": "2021-01-03T05:13:26.384485",
     "exception": false,
     "start_time": "2021-01-03T05:13:25.549725",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Mean meter 0 reading: outlier building #803')\n",
    "print(train_m[(train_m['building_id']==803) & (train_m['meter']==0)]['meter_reading'].mean())\n",
    "print('Mean meter 0  reading: overall')\n",
    "print(train_m[(train_m['meter']==0)]['meter_reading'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.037104,
     "end_time": "2021-01-03T05:13:26.459552",
     "exception": false,
     "start_time": "2021-01-03T05:13:26.422448",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Outliers can produce outsized error terms in a model that fits the rest of the data well. To address this, we will rescale the target variable (meter reading) for each building and meter type. \n",
    "\n",
    "The overall average meter reading is about 2000 so we will rescale to the range (0,2000). Note that the scale we choose has an effect on model training because the evaluation metric (root mean squared log error) is sensitive to the scale of the numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-03T05:13:26.549931Z",
     "iopub.status.busy": "2021-01-03T05:13:26.549253Z",
     "iopub.status.idle": "2021-01-03T05:13:26.602110Z",
     "shell.execute_reply": "2021-01-03T05:13:26.602772Z"
    },
    "papermill": {
     "duration": 0.106197,
     "end_time": "2021-01-03T05:13:26.602949",
     "exception": false,
     "start_time": "2021-01-03T05:13:26.496752",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#We would like to rescale the meter reading column for each building and meter reading to prevent outliers from skewing the reults.\n",
    "#This is a class to achieve that for any chosen groups. It is a modified version of code by Szymon Maszke: \n",
    "#https://stackoverflow.com/questions/55601928/apply-multiple-standardscalers-to-individual-groups\n",
    "from sklearn.base import clone\n",
    "class GroupTargetTransform:\n",
    "    def __init__(self, transformation):\n",
    "        self.transformation = transformation\n",
    "        self._group_transforms = {} #this library will hold the group transforms\n",
    "\n",
    "    def _call_with_function(self, X, y, function: str):\n",
    "        yhat = pd.Series(dtype = 'float32')#this will hold the rescaled target data\n",
    "        X['target'] = pd.Series(y, index=X.index)\n",
    "        for gr in X.groupby(self.features):\n",
    "            n = gr[0] #this is a tuple id for the group\n",
    "            g_X = gr[1] #this is the group dataframe\n",
    "            g_yhat = getattr(self._group_transforms[n], function)(g_X['target'].values.reshape(-1,1))#scale the target variable\n",
    "            g_yhat = pd.Series(g_yhat.flatten(), index = g_X.index)\n",
    "            yhat = yhat.append(g_yhat)\n",
    "        X.drop('target', axis=1, inplace = True)\n",
    "        return yhat.sort_index()\n",
    "    \n",
    "    def fit(self, X, y, features):\n",
    "        self.features = features\n",
    "        X['target'] = pd.Series(y, index=X.index) \n",
    "        for gr in X.groupby(self.features):\n",
    "            n = gr[0] #this is a tuple id for the group\n",
    "            g_X = gr[1] #this is the group dataframe\n",
    "            sc = clone(self.transformation) #create a new instance of the transform\n",
    "            self._group_transforms[n] = sc.fit(g_X['target'].values.reshape(-1,1))\n",
    "        X.drop('target', axis=1, inplace=True)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y):\n",
    "        return self._call_with_function(X, y, \"transform\")\n",
    "\n",
    "    def fit_transform(self, X, y, features):\n",
    "        self.fit(X, y, features)\n",
    "        return self.transform(X, y)\n",
    "\n",
    "    def inverse_transform(self, X, y):\n",
    "        return self._call_with_function(X, y, \"inverse_transform\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-03T05:13:26.721557Z",
     "iopub.status.busy": "2021-01-03T05:13:26.720606Z",
     "iopub.status.idle": "2021-01-03T05:14:28.039236Z",
     "shell.execute_reply": "2021-01-03T05:14:28.038669Z"
    },
    "papermill": {
     "duration": 61.380839,
     "end_time": "2021-01-03T05:14:28.039350",
     "exception": false,
     "start_time": "2021-01-03T05:13:26.658511",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#rescale the target variable for each building and meter type.\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = GroupTargetTransform(MinMaxScaler(feature_range = (0,2000))) #2000 is roughly the average meter reading for all the train data\n",
    "train_m['meter_reading_rescaled'] = scaler.fit_transform(train_m, train_m['meter_reading'], ['building_id', 'meter'])\n",
    "#convert to log(y+1) so the RMSE evaluation metric is actually giving the RMSLE (teh evaluation metric for the competition)\n",
    "train_m['meter_reading_rescaled'] = np.log1p(train_m['meter_reading_rescaled']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.037027,
     "end_time": "2021-01-03T05:14:28.114511",
     "exception": false,
     "start_time": "2021-01-03T05:14:28.077484",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <a id='7'>7. Model Training</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-03T05:14:28.350254Z",
     "iopub.status.busy": "2021-01-03T05:14:28.195926Z",
     "iopub.status.idle": "2021-01-03T05:15:07.079125Z",
     "shell.execute_reply": "2021-01-03T05:15:07.078540Z"
    },
    "papermill": {
     "duration": 38.927402,
     "end_time": "2021-01-03T05:15:07.079235",
     "exception": false,
     "start_time": "2021-01-03T05:14:28.151833",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save test and train data onto the HDD so we can free up some memory\n",
    "import pickle\n",
    "pickle.dump( test_m, open( \"test_m.p\", \"wb\" ) )\n",
    "pickle.dump( train_m, open( \"train_m.p\", \"wb\" ) )\n",
    "del test_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.036948,
     "end_time": "2021-01-03T05:15:07.153850",
     "exception": false,
     "start_time": "2021-01-03T05:15:07.116902",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Below we will tune the model parameters and get an idea of how well the model can perform on unseen data. We have done this by:\n",
    "\n",
    "* Holding out the last 2.5 months of data for validation\n",
    "* Holding out 10% of the buildings for validation\n",
    "* Cross-validating time-series wise (for parameter tuning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-03T05:15:07.238908Z",
     "iopub.status.busy": "2021-01-03T05:15:07.231755Z",
     "iopub.status.idle": "2021-01-03T05:19:23.415817Z",
     "shell.execute_reply": "2021-01-03T05:19:23.415283Z"
    },
    "papermill": {
     "duration": 256.225033,
     "end_time": "2021-01-03T05:19:23.415984",
     "exception": false,
     "start_time": "2021-01-03T05:15:07.190951",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, TimeSeriesSplit\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, mean_absolute_error, mean_squared_log_error\n",
    "\n",
    "#defining a couple of functions for later use\n",
    "def clip(x):\n",
    "    return np.clip(x, a_min=0, a_max=None)\n",
    "def rmse(y, y_pred):\n",
    "    out = np.sqrt(mean_squared_error(clip(y), clip(y_pred)))\n",
    "    return out\n",
    "\n",
    "#prepare training data\n",
    "X = train_m.dropna(subset=['meter_reading']) #drop all rows where the meter reading is not included\n",
    "X = X.sort_values(by=['timestamp'], axis=0) #ensure X is sorted by timstamp for later timeseries cross-validation\n",
    "\n",
    "builds = X['building_id'].unique()#array of building ids in the dataset\n",
    "build_train, build_val = train_test_split(builds, test_size = 0.1, random_state=0)#hold out 10% of the buildings for validation\n",
    "\n",
    "train = X.loc[(X['timestamp']<'2016-10-15') \n",
    "          & (X['building_id'].isin(build_train))] #we will train on only the first 80% of the year and 90% buildings\n",
    "val_t = X.loc[(X['timestamp']>='2016-10-15') & (X['building_id'].isin(build_train))] #rest of the year and same buildings as above\n",
    "val_b = X.loc[(X['building_id'].isin(build_val))] #full year and the rest of the buildings\n",
    "\n",
    "y_train, y_val_t, y_val_b = train['meter_reading_rescaled'], val_t['meter_reading'], val_b['meter_reading'] #extracting the meter reading as our target variable\n",
    "X_train, X_val_t, X_val_b = train.drop(['meter_reading', 'meter_reading_rescaled', 'timestamp'], axis=1), val_t.drop(['meter_reading', 'meter_reading_rescaled','timestamp'], axis=1), val_b.drop(['meter_reading','meter_reading_rescaled','timestamp'], axis=1)\n",
    "\n",
    "del X, train, val_t, val_b #no longer needed - free up memory\n",
    "\n",
    "# lgbm model\n",
    "model = LGBMRegressor(\n",
    "num_leaves = 600,\n",
    "min_data_in_leaf = 50,\n",
    "random_state = 0\n",
    ")\n",
    "\n",
    "#cross-validation for paramter tuning\n",
    "# params = {\n",
    "#     'num_leaves': [600],#add values to these lists to run a parmaeter optimization. These were found to be optimum.\n",
    "#          }\n",
    "\n",
    "# #define a rmse scorer for gridsearchcv\n",
    "# rmse_scorer = make_scorer(rmse, greater_is_better=False)\n",
    "# #split training data time series-wise for cross-validation\n",
    "# tscv = TimeSeriesSplit(n_splits=3)\n",
    "# #grid search\n",
    "# #Note that the scores given are based on the rescaled meter readings, so are not a direct representation of model performance\n",
    "# for model_name, grid in params.items():\n",
    "#     searchCV = GridSearchCV(model, scoring=rmse_scorer, cv=tscv, param_grid=params)\n",
    "#     print('GridSearchCV fitting...')\n",
    "#     searchCV.fit(X_train, y_train)\n",
    "#     scores = -1*searchCV.cv_results_['mean_test_score']\n",
    "#     params = searchCV.cv_results_['params']\n",
    "#     for i in range(0, len(scores)):\n",
    "#       print(params[i], '->', scores[i])\n",
    "\n",
    "#Evaluate combined model on the ramining validation data\n",
    "print('Fitting...')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print('Time predictions...')\n",
    "preds = clip(model.predict(X_val_t)) #make time predictions\n",
    "preds_inv = scaler.inverse_transform(X_val_t, np.expm1(preds)) #convert back to original scale, remembering to invert the log transform\n",
    "y_val_t = y_val_t.sort_index()\n",
    "score = mean_absolute_error(preds_inv, y_val_t)\n",
    "print('Mean absolute error - time prediction:', score)\n",
    "RMSLE = np.sqrt(mean_squared_log_error(preds_inv, y_val_t))\n",
    "print('RMSLE - time prediction:', RMSLE)\n",
    "\n",
    "print('Building predictions...')\n",
    "preds = clip(model.predict(X_val_b))\n",
    "preds_inv = scaler.inverse_transform(X_val_b, np.expm1(preds))\n",
    "y_val_b = y_val_b.sort_index()\n",
    "score = mean_absolute_error(preds_inv, y_val_b)\n",
    "print('Mean absolute error - new buildings:', score)\n",
    "RMSLE = np.sqrt(mean_squared_log_error(preds_inv, y_val_b))\n",
    "print('RMSLE - new buildings:', RMSLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.039173,
     "end_time": "2021-01-03T05:19:23.494831",
     "exception": false,
     "start_time": "2021-01-03T05:19:23.455658",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now that we have tuned the model parameters and have an idea of model performance. We will fit on the entire training dataset so we have as much information as possible for the final test set prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-03T05:19:23.608441Z",
     "iopub.status.busy": "2021-01-03T05:19:23.607819Z",
     "iopub.status.idle": "2021-01-03T05:22:28.993793Z",
     "shell.execute_reply": "2021-01-03T05:22:28.994426Z"
    },
    "papermill": {
     "duration": 185.460082,
     "end_time": "2021-01-03T05:22:28.994580",
     "exception": false,
     "start_time": "2021-01-03T05:19:23.534498",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#prepare training data\n",
    "train = train_m.dropna(subset=['meter_reading']) #drop all rows where the meter reading is not included\n",
    "\n",
    "y_train = train['meter_reading_rescaled'] #extracting the meter reading as our target variable\n",
    "X_train = train.drop(['meter_reading', 'meter_reading_rescaled', 'timestamp'], axis=1)\n",
    "\n",
    "del train, train_m, X_val_t #no longer needed - free up memory\n",
    "gc.collect()\n",
    "\n",
    "#Fitting on all training data\n",
    "print('Final Fitting...')\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.049648,
     "end_time": "2021-01-03T05:22:29.094784",
     "exception": false,
     "start_time": "2021-01-03T05:22:29.045136",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <a id='8'>8. Model Predictions</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-03T05:22:29.201448Z",
     "iopub.status.busy": "2021-01-03T05:22:29.200531Z",
     "iopub.status.idle": "2021-01-03T05:22:34.734616Z",
     "shell.execute_reply": "2021-01-03T05:22:34.734055Z"
    },
    "papermill": {
     "duration": 5.596168,
     "end_time": "2021-01-03T05:22:34.734726",
     "exception": false,
     "start_time": "2021-01-03T05:22:29.138558",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#free up memory then load in the test data\n",
    "del X_train, y_train\n",
    "gc.collect()\n",
    "X_test = pickle.load( open( \"test_m.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-03T05:22:34.829217Z",
     "iopub.status.busy": "2021-01-03T05:22:34.825454Z",
     "iopub.status.idle": "2021-01-03T05:30:16.594387Z",
     "shell.execute_reply": "2021-01-03T05:30:16.593283Z"
    },
    "papermill": {
     "duration": 461.819216,
     "end_time": "2021-01-03T05:30:16.594744",
     "exception": false,
     "start_time": "2021-01-03T05:22:34.775528",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#output the final predictions on the test data to a csv file\n",
    "preds = np.empty(len(X_test))#we will predict in three steps to save memory\n",
    "print('A')\n",
    "preds[:int(len(X_test)/3)] = model.predict(X_test.drop(['row_id', 'timestamp'], axis=1).iloc[:int(len(X_test)/3)])\n",
    "preds[int(len(X_test)/3):int(len(X_test)*2/3)] = model.predict(X_test.drop(['row_id', 'timestamp'], axis=1).iloc[int(len(X_test)/3):int(len(X_test)*2/3)])\n",
    "preds[int(len(X_test)*2/3):] = model.predict(X_test.drop(['row_id', 'timestamp'], axis=1).iloc[int(len(X_test)*2/3):])\n",
    "final_predictions = scaler.inverse_transform(X_test, np.expm1(preds))\n",
    "X_test = X_test.sort_index()\n",
    "output = pd.DataFrame({'row_id': X_test['row_id'], 'meter_reading': clip(final_predictions)})\n",
    "output['meter_reading'] = output['meter_reading'].round(decimals=4)#to save space\n",
    "output.to_csv('sub.csv', index_label = 'row_id', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.041395,
     "end_time": "2021-01-03T05:30:16.682591",
     "exception": false,
     "start_time": "2021-01-03T05:30:16.641196",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As a final check, we can plot the model predictions with the existing data for a specific building and meter type. Everything during 2016 is real data while the rest is our final predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-03T05:30:16.777481Z",
     "iopub.status.busy": "2021-01-03T05:30:16.775465Z",
     "iopub.status.idle": "2021-01-03T05:30:52.865847Z",
     "shell.execute_reply": "2021-01-03T05:30:52.865272Z"
    },
    "papermill": {
     "duration": 36.141793,
     "end_time": "2021-01-03T05:30:52.865978",
     "exception": false,
     "start_time": "2021-01-03T05:30:16.724185",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "b_id = 400 #building id\n",
    "m_id = 0 #meter id\n",
    "\n",
    "train_m = pickle.load( open('train_m.p', 'rb'))\n",
    "building_current = train_m.loc[(train_m['building_id']==b_id) & (train_m['meter']==m_id)]\n",
    "building_forecast = X_test.loc[(X_test['building_id']==b_id) & (X_test['meter']==m_id)].merge(output, how='left', on = ['row_id'], validate='one_to_one')\n",
    "building = pd.concat([building_current, building_forecast])\n",
    "\n",
    "X_o = building.drop(['meter_reading', 'row_id', 'timestamp', 'site_id'], axis=1)\n",
    "y_o = building['meter_reading']\n",
    "\n",
    "mod_plot = pd.DataFrame(data={#'meter_reading (predicted)':building_forecast['meter_reading'],\n",
    "                                    'meter_reading (actual and predicted)':y_o.values},\n",
    "                                    index=building['timestamp'])\n",
    "start_time = '2016-01-01'\n",
    "end_time = '2019-01-01'\n",
    "mod_plot = mod_plot.loc[(start_time<mod_plot.index)&(mod_plot.index<end_time)].resample('D').mean()\n",
    "mod_plot.plot(rot=45)#plot each model vs target"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "papermill": {
   "duration": 1239.777738,
   "end_time": "2021-01-03T05:30:53.019089",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-01-03T05:10:13.241351",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
